{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d636973d-c27d-493e-8a27-d532bff1d90c",
   "metadata": {},
   "source": "## Assignment Week 3 - grk"
  },
  {
   "cell_type": "markdown",
   "id": "20d4b374-45a0-4676-a8c9-db16b182c84f",
   "metadata": {},
   "source": [
    "#### Part 1: Using the TextBlob Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "id": "d81ec862-79de-465f-ae55-ca8f38a81d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:46:39.563717Z",
     "start_time": "2025-03-08T00:46:39.559845Z"
    }
   },
   "source": [
    "# import modules\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "51d0316e-04fb-4320-9295-dff5497a27b3",
   "metadata": {},
   "source": [
    "Import the movie review data as a data frame and ensure that the data is loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "id": "65ba9545-c63c-44a8-b79e-7fab4055c415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:46:39.999304Z",
     "start_time": "2025-03-08T00:46:39.711503Z"
    }
   },
   "source": [
    "# read the dataset as a dataframe\n",
    "df = pd.read_csv(\"./assignments-data/labeledTrainData.tsv\", sep=\"\\t\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "ce00e355-0f51-4718-8c93-2297232067e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:46:40.007066Z",
     "start_time": "2025-03-08T00:46:39.999304Z"
    }
   },
   "source": [
    "# view few rows to check dataset is loaded correcly\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "4200a859-bbc3-4acb-b342-bfbce605e929",
   "metadata": {},
   "source": [
    "How many of each positive and negative reviews are there?"
   ]
  },
  {
   "cell_type": "code",
   "id": "f0a4f451-aa19-4a82-a8a2-93cd97f02786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:46:40.018208Z",
     "start_time": "2025-03-08T00:46:40.007066Z"
    }
   },
   "source": [
    "# using value counts to get counts of postive and negetive sentiments\n",
    "df[\"sentiment\"].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    12500\n",
       "0    12500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "5422f512-d8a0-4afc-ab93-1cdaff5cb0f8",
   "metadata": {},
   "source": [
    "Use TextBlob to classify each movie review as positive or negative. Assume that a polarity score greater than or equal to zero is a positive sentiment and less than 0 is a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "id": "60f63d39-d3e4-47ed-9c43-f5e8e80973fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:46:40.024005Z",
     "start_time": "2025-03-08T00:46:40.018208Z"
    }
   },
   "source": [
    "# this function converts input text to lower case and then performs sentiment analysis using TextBlob sentiment analyzer\n",
    "# this function returns 1 (positive) if polarity of TextBlob sentiment analysis is greater than 0 otherwise returns 0 (negative)\n",
    "def perform_sentiment_analysis(text):\n",
    "    testimonial = TextBlob(text.lower())\n",
    "    return 0 if testimonial.sentiment.polarity < 0 else 1"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "9ebf2ee1-b4d7-4ac2-bf94-47886bb6eb2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:47:01.760073Z",
     "start_time": "2025-03-08T00:46:40.025325Z"
    }
   },
   "source": [
    "# perform sentiment analysis for each review and store in a new column\n",
    "df[\"textblob_sentiment\"] = df[\"review\"].apply(perform_sentiment_analysis)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "25784ac9-5255-4023-8c37-542b711ab26c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:47:01.769665Z",
     "start_time": "2025-03-08T00:47:01.760591Z"
    }
   },
   "source": [
    "# view few rows\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "   textblob_sentiment  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "38132f9c-9a40-4fec-b980-b1fe72def70b",
   "metadata": {},
   "source": [
    "Check the accuracy of this model. Is this model better than random guessing?"
   ]
  },
  {
   "cell_type": "code",
   "id": "e977363b-871f-4d87-9529-cf230e239bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:47:01.773363Z",
     "start_time": "2025-03-08T00:47:01.769665Z"
    }
   },
   "source": [
    "# importing accuracy score module for calculating accuracy\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "eadedb05-978b-435d-9934-5e23e0f3694e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:47:01.779830Z",
     "start_time": "2025-03-08T00:47:01.773363Z"
    }
   },
   "source": [
    "# get accuracy score for sentiment generated by TextBlob against the labeled data\n",
    "accuracy_score(df[\"sentiment\"],df[\"textblob_sentiment\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68552"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "4b1038f1-2e68-48ca-a900-64133bbe49b8",
   "metadata": {},
   "source": [
    "The sentiment analyzer model using `TextBlob` has an accuracy of $68.5$%. The original labeleled dataset has has $50$% positive and $50$% negetive sentiment. Hence, with random guessing we can expect to get a maximum of $50$% accuracy. However, the `TextBlob` sentiment analyzer model has accuracy higher than the $50$%, therefore we can say that the `TextBlob` model is better than _random guessing_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda02db9-931a-42fb-ae8b-12dd128257ad",
   "metadata": {},
   "source": [
    "For up to five points extra credit, use another prebuilt text sentiment analyzer, e.g., VADER, and repeat steps (3) and (4)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9d31e8a-dc78-480e-bdb2-5f3ef28fafa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:47:01.782895Z",
     "start_time": "2025-03-08T00:47:01.779830Z"
    }
   },
   "source": [
    "# import VADER sentiment analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "f3aa0b6d-8bbe-49e1-b5a9-e491bac1a897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:47:01.789857Z",
     "start_time": "2025-03-08T00:47:01.782895Z"
    }
   },
   "source": [
    "# this function performs VADER sentiment analysis on the input text and returns 1 (poitive) \n",
    "# if the compound score from the analysis is greater than eqaul 0 otherwise 0 (negative)\n",
    "def perform_sentiment_analysis_using_vader(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = analyzer.polarity_scores(text.lower())\n",
    "    return 0 if sentiment_dict[\"compound\"] < 0 else 1"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "29058de4-2b8c-47aa-9a74-1c979525ee8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:49.613330Z",
     "start_time": "2025-03-08T00:47:01.789857Z"
    }
   },
   "source": [
    "# perform VADER sentiment analysis for each review using above function and store in a new column\n",
    "df[\"sentiment_with_vader\"] = df[\"review\"].apply(perform_sentiment_analysis_using_vader)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "eda420f0-9dec-4234-9acc-0e9846d541fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:49.620979Z",
     "start_time": "2025-03-08T00:50:49.613330Z"
    }
   },
   "source": [
    "# view few rows\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "   textblob_sentiment  sentiment_with_vader  \n",
       "0                   1                     0  \n",
       "1                   1                     1  \n",
       "2                   0                     0  \n",
       "3                   1                     0  \n",
       "4                   0                     1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "      <th>sentiment_with_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "e88a40ff-e823-48a7-90c3-75020c92e8fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:49.631079Z",
     "start_time": "2025-03-08T00:50:49.620979Z"
    }
   },
   "source": [
    "# get accuracy score for sentiment generated by VADER against the labeled data\n",
    "accuracy_score(df[\"sentiment\"],df[\"sentiment_with_vader\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69404"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "01c07cee-d2c0-48b4-9529-feda55210793",
   "metadata": {},
   "source": [
    "We see that the accuracy of _Sentiment analysis_ using `VADER` sentiment analyzer did not change much from `TextBlob` sentiment analyzer, just $1$% increase in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca1611-8171-413a-9c95-c7215c020393",
   "metadata": {},
   "source": [
    "#### Part 2: Prepping Text for a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "192ad05b-9f6a-487c-936c-11cd2c15be0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:49.636207Z",
     "start_time": "2025-03-08T00:50:49.631404Z"
    }
   },
   "source": [
    "# import modules\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "a5e3bef1-2537-4e4b-8134-fbdc9c612224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:49.953864Z",
     "start_time": "2025-03-08T00:50:49.636207Z"
    }
   },
   "source": [
    "# read the dataset into a dataframe\n",
    "df = pd.read_csv(\"./assignments-data/labeledTrainData.tsv\", sep=\"\\t\")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "fa77082e-eed0-41fa-bbd9-c491a82e64cf",
   "metadata": {},
   "source": [
    "Convert all text to lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "id": "5b13c89e-4306-4173-bdd2-919df6465ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:49.999293Z",
     "start_time": "2025-03-08T00:50:49.953864Z"
    }
   },
   "source": [
    "# convert reviews to lower case and store in a new column\n",
    "df[\"cleaned_review\"] = df[\"review\"].str.lower()"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "b83d702c-895d-4439-87b9-62842e8b3499",
   "metadata": {},
   "source": [
    "Remove punctuation and special characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "id": "5ff11a9e-7c24-49b0-a3e9-2ba9af812738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:50.336649Z",
     "start_time": "2025-03-08T00:50:49.999293Z"
    }
   },
   "source": [
    "# this function clean punctuation and symbols from input text\n",
    "punctuation = dict.fromkeys(\n",
    "[i for i in range(sys.maxunicode)\n",
    "if unicodedata.category(chr(i)).startswith(\"P\") or unicodedata.category(chr(i)).startswith(\"S\")],\n",
    "    None\n",
    " )\n",
    "def remove_punctuation_symbols(text):\n",
    "    return text.translate(punctuation)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "411155bb-7fbe-4c81-bef9-9594038aa417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:50.881435Z",
     "start_time": "2025-03-08T00:50:50.338538Z"
    }
   },
   "source": [
    "# remove punctuation and symbols from reviews using the above function \n",
    "df[\"cleaned_review\"] = df[\"cleaned_review\"].apply(remove_punctuation_symbols)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "8b46098d-ab58-445b-b455-90dba3983981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T00:50:50.981137Z",
     "start_time": "2025-03-08T00:50:50.881435Z"
    }
   },
   "source": [
    "# this function removes stopwords from input text\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def remove_stopwords(text):\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    return \" \".join([word for word in tokenized_words if word not in stop_words])"
   ],
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gyanr/nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 84\u001B[0m     root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    578\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 579\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords.zip/stopwords/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gyanr/nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# this function removes stopwords from input text\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m \u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mremove_stopwords\u001B[39m(text):\n\u001B[0;32m      4\u001B[0m     tokenized_words \u001B[38;5;241m=\u001B[39m word_tokenize(text)\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 120\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     84\u001B[0m             root \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[0;32m     89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 81\u001B[0m         root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    577\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    578\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 579\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\gyanr/nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\gyan-python-workspace\\\\jup-workspace\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\gyanr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "f01ba4c7-3827-4595-9cf4-478812c7d3d4",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "id": "ccfae8d2-4e70-444a-8779-d2ef14a63e97",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-08T00:50:50.981137Z"
    }
   },
   "source": [
    "# remove stopwords from reviews using above function\n",
    "df[\"cleaned_review\"] = df[\"cleaned_review\"].apply(remove_stopwords)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ee850ec-09ac-4930-9351-e5642fab874e",
   "metadata": {},
   "source": [
    "Apply NLTKâ€™s PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "id": "2733f00a-d107-4994-85ff-cdcd220b993b",
   "metadata": {},
   "source": [
    "# this function perform stemming on the input text using NLTK PorterStemmer\n",
    "def porter_stemmer(text):\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    return \" \".join([porter.stem(words) for words in tokenized_words])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a13c690-bac8-4f73-8db8-73dc029fb20d",
   "metadata": {},
   "source": [
    "# apply stemming on reviews using above function\n",
    "df[\"cleaned_review\"] = df[\"cleaned_review\"].apply(porter_stemmer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc493a53-dea9-4751-a9bb-6d1ad4df3071",
   "metadata": {},
   "source": [
    "# view few rows after the above text preparation\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4b0292b-0735-46a2-b85a-4c45c60aece1",
   "metadata": {},
   "source": [
    "Create a bag-of-words matrix from your stemmed text (output from (4)), where each row is a word-count vector for a single movie review (see sections 5.3 & 6.8 in the Machine Learning with Python Cookbook). Display the dimensions of your bag-of-words matrix. The number of rows in this matrix should be the same as the number of rows in your original data frame."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d80db33-ba1e-480e-b09a-f7ba3669767e",
   "metadata": {},
   "source": [
    "# Generate bag of words matrix using CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "bag_of_words = count_vec.fit_transform(df[\"cleaned_review\"])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "695cd9de-3efb-4967-9bac-b39830cb49ea",
   "metadata": {},
   "source": [
    "# dimensions of the generated bag-of-words matrix\n",
    "bag_of_words.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c89568ee-86b2-4af5-bb6b-ba08541d66f9",
   "metadata": {},
   "source": [
    "# dimensions of the dataframe\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0bb54890-f7b9-4406-90d5-08e39c92f722",
   "metadata": {},
   "source": [
    "we see that the number of rows of the bag-of-words matrix is same as the number of rows in the original dataframe i.e $25000$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a50c5-3624-4da1-aea2-3e231f603932",
   "metadata": {},
   "source": [
    "Create a term frequency-inverse document frequency (tf-idf) matrix from your stemmed text, for your movie reviews (see section 6.9 in the Machine Learning with Python Cookbook). Display the dimensions of your tf-idf matrix. These dimensions should be the same as your bag-of-words matrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc829b28-b002-4008-a11b-e7174a12de4c",
   "metadata": {},
   "source": [
    "# generate term frequency-inverse document frequency (tf-idf) matrix using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_feature_matrix = tfidf.fit_transform(df[\"cleaned_review\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c45ce101-44a6-432e-b2cd-43ebf2cb171a",
   "metadata": {},
   "source": [
    "# dimensions of te tf-idf feature matrix\n",
    "tfidf_feature_matrix.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10eaefaa-d4d8-468b-9e71-ceac135fcb1f",
   "metadata": {},
   "source": [
    "we see that the number of rows of the tf-idf matrix is same as the number of rows in the original dataframe i.e.$25000$"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b064442-3f3e-4511-b633-aacd68d25b7f",
   "metadata": {},
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24dd887d-dac1-4c26-ad6b-464485c83915",
   "metadata": {},
   "source": [
    "df[(df[\"sentiment\"] == 1) & (df[\"textblob_sentiment\"] == 1)].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce234eec-9f4d-4bbd-9b05-a77064ee7c92",
   "metadata": {},
   "source": [
    "df[(df[\"sentiment\"] == 0) & (df[\"textblob_sentiment\"] == 1)].shape"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
